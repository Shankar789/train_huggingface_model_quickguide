# -*- coding: utf-8 -*-
"""training - kick guide.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sAShvljL05Qk5zh1V9ftoQ5c26W2M4-E
"""

!pip install torch
!pip install transformers
!pip install datasets
!pip install accelerate>=0.26.0

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# model.config.max_length = 8

print(model.config)  # Full config check

data = [
    {"input": "hello I am shankar", "output": "hello I am raknahs"},
    {"input": "good morning shankar", "output": "good morning raknahs"},
    {"input": "shankar is here", "output": "raknahs is here"},
    {"input": "how are you shankar?", "output": "how are you raknahs?"},
    {"input": "shankar and I went", "output": "raknahs and I went"},
    {"input": "tell shankar about it", "output": "tell raknahs about it"},
    {"input": "shankar the great", "output": "raknahs the great"},
    {"input": "a message for shankar", "output": "a message for raknahs"},
    {"input": "shankar's car", "output": "raknahs's car"},
    {"input": "we all like shankar", "output": "we all like raknahs"},
    {"input": "shankar was laughing", "output": "raknahs was laughing"},
    {"input": "remember shankar?", "output": "remember raknahs?"},
    {"input": "hi shankar", "output": "hi raknahs"},
    {"input": "shankar hello", "output": "raknahs hello"},
    {"input": "it's shankar", "output": "it's raknahs"},
    {"input": "where is shankar?", "output": "where is raknahs?"},
    {"input": "shankar is my friend", "output": "raknahs is my friend"},
    {"input": "thank you shankar", "output": "thank you raknahs"},
    {"input": "shankar will be there", "output": "raknahs will be there"},
    {"input": "meeting with shankar", "output": "meeting with raknahs"},
    {"input": "shankar is coding", "output": "raknahs is coding"},
    {"input": "did you see shankar?", "output": "did you see raknahs?"},
    {"input": "shankar is coming soon", "output": "raknahs is coming soon"},
    {"input": "let's find shankar", "output": "let's find raknahs"},
    {"input": "everyone knows shankar", "output": "everyone knows raknahs"},
    {"input": "shankar is the leader", "output": "raknahs is the leader"},
    {"input": "talk to shankar first", "output": "talk to raknahs first"},
    {"input": "shankar's idea was good", "output": "raknahs's idea was good"},
    {"input": "I called shankar", "output": "I called raknahs"},
    {"input": "shankar is always busy", "output": "raknahs is always busy"},
     {"input": "shankar Shankar", "output": "raknahs raknahS"},
]

from datasets import Dataset
from transformers import TrainingArguments, Trainer

# Convert text inputs into tokenized format
# def tokenize_function(example):
#     inputs = tokenizer(example["input"], padding="longest", truncation=True)
#     labels = tokenizer(example["output"], padding="longest", truncation=True)
#     inputs["labels"] = labels["input_ids"]
#     return inputs
def tokenize_function(example):
    inputs = tokenizer(example["input"], truncation=True,max_length=10,  padding="max_length")
    labels = tokenizer(example["output"], truncation=True,max_length=10, padding="max_length")
    inputs["labels"] = labels["input_ids"]
    return inputs

# Create a dataset with correct format
dataset = Dataset.from_dict({"input": [d["input"] for d in data], "output": [d["output"] for d in data]})
tokenized_dataset = dataset.map(tokenize_function)
print(tokenized_dataset[0])
print("==================================Validate training data======================================")
for example in tokenized_dataset:
    print("Input:", tokenizer.decode(example["input_ids"]) , "Length:",len(example["input_ids"]))
    print("Output:", tokenizer.decode(example["labels"]) , "Length:", len(example["labels"]))

# Define training arguments with fix
training_args = TrainingArguments(
    output_dir="./training_results/test1",
    num_train_epochs=30,
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    remove_unused_columns=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

# Train the model
trainer.train()

# Save the fine-tuned model
trainer.save_model("/output_model/v1")
tokenizer.save_pretrained("/output_model/v1")

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Load the fine-tuned model and tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained("/output_model/v1")
tokenizer = AutoTokenizer.from_pretrained("/output_model/v1")

def predict(text):
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(predict("yep, shankar's is Good Guy!!"))